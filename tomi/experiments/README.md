 Experiments
---
This directory contains all the scripts that we conduct our experiments, including 
our method (**TOMI**), baseline method (**MusicGen**), and two ablations (**Standalone LLM - TOMI w/o 
Composition Links** and **Random - TOMI w/o LLM**), as described in our paper. Here is the description 
of each file:

1. `tomi_llm_request.py`: This file contains the implementation of the class `TOMILLMRequest`, which establish the connection and communication to the 
foundation LLM, currently we support models from OpenAI and Anthropic. For model API baseUrl and token settings, 
please open `config.py` in the project root directory and change the corresponding settings. `TOMILLMRequest` sends 
prompts to the LLM server and receive the model response in JSON format; it also checks the output for invalid values and 
returns the arrangement data until no errors found. A `StandaloneLLMRequest` class is also implemented for "Standalone LLM" method.
2. `tomi_song.py`: This file converts the song arrangement (JSON data) generated by the LLM to node instances and parses composition links.
3. `standalone_llm_song.py`: This file contains the implementation of the ablation "Standalone LLM - TOMI w/o Composition Links". We redesign the prompt to 
let the LLM generate a sequence of tracks and clip descriptions with position information (time point and track location) conditioned on 
a section sequence. The sample retrieval mechanism is also applied for clips.
4. `random_song.py`: This file contains the implementation of the ablation "Random - TOMI w/o LLM". We replace the LLM operations in our system with a 
rule-based method that uses randomized operations to generate music within the composition-links structure.
5. `musicgen.py`: We use the MusicGen-Large-3.3B model as the baseline with prompts specifying key, tempo, and section sequence. We 
apply a sliding window approach to generate audio longer than the 30s limit. We modify its inference process to include the current 
generationâ€™s position within the full composition and its corresponding phrase notations in the prompt at each step, guiding the model 
to align its output with the given structure. **Note:** to run this file, you need to create a new python/conda environment with Python 
3.9 and PyTorch 2.1.0, and install Meta's [AudioCraft](https://github.com/facebookresearch/audiocraft/tree/main).
6. `llm_prompts.py`: This file contains the system prompt and default user prompt for `TOMILLMRequest` and `StandaloneLLMRequest`.
7. `experiments.py`: This file contains the script to generate all experiment results for TOMI, Standalone LLM, and Random. You have to 
open REAPER before running the script, and you will need to manually render the audio within REAPER for each generation to get the audio 
results.
8. `experiment_section_patterns.py`: This file contains the four defined song structure patterns for our experiments.
9. `ils_metric.py`: This file contains our refined version of the ILS metric, which was first introduced in [here](https://github.com/ZZWaang/whole-song-gen).
10. `simple_lofi_song.py`: This file provides a simple song arrangement template using TOMI data structure.

**Note:** We have updated TOMI with a new transformation node type: [Fill Transform](../nodes/transformation/fill_transform_node.py). 
Previously, we used the Fx Transform for placing drum fill clips at the end of a section. However, this approach lacked flexibility and 
could lead to beat misalignments. To address this, we've introduced the Fill Transform, specifically designed for handling short drum 
fills. It simplifies loop control for fill clips, for example, repeating a fill every 8 bars.
